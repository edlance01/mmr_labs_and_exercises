{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying and Ingesting Your Data**\n",
    "\n",
    "In this exercise:  \n",
    "We'll continue to work with our document.  \n",
    "Step 1: Extract our data into paragraphs.  \n",
    "Step 2: Split the paragraphs into chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extract our data into paragraphs.  \n",
    "\n",
    "Remember that the LLM will tokenize our content.  Should we break it into pages, paragraph, sentences, or something else?   (Could even convert to HTML and then parse.)  \n",
    "[Tokenizer Example](../static/images/tokenizer.png)  \n",
    "[Python Docx API](https://python-docx.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document as DocxDocument\n",
    "\n",
    "file_path = \"../static/input_files/Jupyter_Notebook_Info.docx\"\n",
    "doc = DocxDocument(file_path)\n",
    "paragraph_chunks = []\n",
    "\n",
    "# Extract text content from paraAgraphs\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.strip():\n",
    "        paragraph_chunks.append(para.text)\n",
    "        # print(f\"PARAGRAPH:{para.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Split the paragraphs into chunks.  \n",
    "Discuss: What's the best way?  \n",
    "\n",
    "[Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# We join all the text into a single string before splitting\n",
    "text_chunks = text_splitter.split_text(\"\\n\".join(paragraph_chunks))\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    print(f\"CHUNK:{chunk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
