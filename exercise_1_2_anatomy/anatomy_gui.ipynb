{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  Do we want to add text splitting? e.g. \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500, chunk_overlap=50\n",
    "        )\n",
    "        text_chunks_docx = text_splitter.split_text(\"\\n\".join(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE OVERVIEW\n",
    "\n",
    "In this exercise, we'll step through:\n",
    "A. Chunking a document into paragraphs\n",
    "B. Storing all of the raw paragraph text in our database\n",
    "C. After everything is stored as text in the database, we'll retrieve those paragraphs and embed/vectorize each one, storing it in the vector store (pgvector from Postgres in our case).\n",
    "D. Then, we'll act as a user and send in a query, that will also be vectorized by the same tool we used to vectorize our paragraphs.\n",
    "E. Then we'll find and retrieve the vectors that are most similar to our query.\n",
    "F. Once we have the similar vectors, we'll submit them, along with the user query to our LLM and print our response.\n",
    "\n",
    "\n",
    "NOTE: When you setup directories for labs, you may have to run this code to add the virtual environment as a kernel (otherwise, things worked but you will get some warnings).\n",
    "python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Import docx library for Python and instantiate a document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document as DocxDocument\n",
    "doc = DocxDocument(\"Jupyter_Notebook_Info.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Extract and chunk into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                doc_chunks.append(para.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Connect to the database \n",
    "(Database has been created for you.  See create_lame_db.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "def get_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "                dbname=\"lame_db\",\n",
    "                user=\"postgres\",\n",
    "                password=\"admin\",\n",
    "                host=\"localhost\",\n",
    "                port=\"5432\",\n",
    "        )\n",
    "    except (psycopg2.DatabaseError, Exception) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "    \n",
    "    return conn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Store the chunks in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import sql\n",
    "\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    with conn.cursor() as cursor:\n",
    "        # Insert file metadata and content into the complete_files table\n",
    "        for chunk in doc_chunks:\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"INSERT INTO text_chunks (text) VALUES (%s) RETURNING id\"),\n",
    "                [chunk],\n",
    "            )\n",
    "            pk = cursor.fetchone()[0]  # (Optional) capture the returned primary key\n",
    "    \n",
    "            conn.commit()  # Commit the transaction after each insert\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting file with chunks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Vectorization\n",
    "A. Retrieve Current Chunks from Database\n",
    "B. Create embedding of each chunk\n",
    "C. Store vector of embedding in pgvector (vector store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Retreive Currrent Chunks from Database\n",
    "current_chunks = []\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        sql.SQL(\"SELECT id, text FROM text_chunks WHERE is_vectorized = FALSE\"),\n",
    "    )\n",
    "\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        current_chunks.append(row) # append tuple of id and text \n",
    "        cursor.execute(f\"UPDATE text_chunks SET is_vectorized = TRUE WHERE id = %s\",\n",
    "                       (row[0],))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Get the embedding model\n",
    "openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=openai_key)\n",
    "\n",
    "# Generate the embedding for each chunk\n",
    "vector_dict = {}\n",
    "for chunk in current_chunks:\n",
    "     content = openai_embedding.embed_query(chunk[1])\n",
    "     # Convert the embedding values to floats (ensures compatibility with storage formats)\n",
    "     float_content = [float(x) for x in content]\n",
    "     vector_dict[chunk[0]] = float_content\n",
    "\n",
    "\n",
    "# add the vectorized content to the vector store\n",
    "with conn.cursor() as cursor:\n",
    "    for cid, vec in vector_dict.items():\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"INSERT INTO mmr_vector (vector, text_chunk_id) VALUES (%s, %s)\"),\n",
    "            [vec,cid]\n",
    "        )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 - Vectorize Incoming Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# query = \"Where should I point my web browser after Jupyter is running?\"\n",
    "# query = \"What is the command to install jupyter?\"\n",
    "# query = \"You can execute a cell by clicking on it and pressing what?\"\n",
    "query = \"What City and State had the highest temperature?\"\n",
    "\n",
    "vectorized_query = openai_embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 - Find Similar Vectors\n",
    "\n",
    "pgvector similarity search operators: \n",
    "<->:\n",
    "Represents the Euclidean distance between two vectors, which is the \"straight-line\" distance between them in multi-dimensional space. \n",
    "<=>:\n",
    "Calculates the cosine similarity between vectors, which is often preferred for high-dimensional data as it focuses on the angle between vectors rather than their magnitude. \n",
    "<#>\n",
    ": Computes the inner product of two vectors, where each corresponding element is multiplied and summed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"vectorized query: {vectorized_query[:5]}\")\n",
    "top_k = 3\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\n",
    "        sql.SQL(\n",
    "            \"\"\"SELECT id, text_chunk_id, 1 - (vector <=> %s::VECTOR) AS similarity\n",
    "               FROM mmr_vector\n",
    "               ORDER BY similarity DESC\n",
    "               LIMIT %s\"\"\"\n",
    "        ),\n",
    "        [vectorized_query, top_k],\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    similar_chunk_ids = []\n",
    "    if rows:\n",
    "        for row in rows:\n",
    "            similar_chunk_ids.append(row[1])\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9 - Get the text chunks of the closest matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    similar_context = []\n",
    "    for chunk_id in similar_chunk_ids:\n",
    "        cur.execute(\n",
    "            sql.SQL(\"\"\"SELECT text FROM text_chunks where id = %s\"\"\"),\n",
    "            [chunk_id],\n",
    "        )\n",
    "        row = cur.fetchone()  # Fetch only one row for the current chunk_id\n",
    "        similar_context.append(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Retrieval\n",
    "Submit Similar Vectors to LLM with query to retrieve a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CONTEXT ITEM:Example: Temperature Table'\n",
      "'CONTEXT ITEM:A: Insert a new cell above the current cell.'\n",
      "'CONTEXT ITEM:Ctrl+Shift+Space: Scroll up.'\n",
      "\n",
      "\n",
      "\n",
      "('RESPONSE:The provided context does not include information about which city '\n",
      " 'and state had the highest temperature.')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "# Show the similar content retrieved\n",
    "for sc in similar_context:\n",
    "    pprint(f\"CONTEXT ITEM:{sc}\")\n",
    "\n",
    "# Format the prompt\n",
    "prompt = f\"\"\"You are an assistant for question-answering tasks. Use only \n",
    "the following pieces of retrieved context to answer the \n",
    "question. Use 3 sentences maximum to keep your answer concise. Here's a query: \n",
    "{query} and here are similar queries of retrieved context: {similar_context}. Again,\n",
    "only base your answer on the similar queries data within the similar context.\"\"\"\n",
    "\n",
    "# Call the OpenAI ChatCompletion API using the updated method\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "print(\"\\n\\n\")\n",
    "pprint(f\"RESPONSE:{response.choices[0].message.content.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
