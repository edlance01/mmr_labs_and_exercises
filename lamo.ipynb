{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document as DocxDocument\n",
    "doc = DocxDocument(\"acme_bank_consolidate_performance_report.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and chunk paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                doc_chunks.append(para.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database \n",
    "(Database has been created for you.  See create_lame_db.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "def get_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "                dbname=\"lame_db\",\n",
    "                user=\"postgres\",\n",
    "                password=\"admin\",\n",
    "                host=\"localhost\",\n",
    "                port=\"5432\",\n",
    "        )\n",
    "    except (psycopg2.DatabaseError, Exception) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "    \n",
    "    return conn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the chunks in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import sql\n",
    "\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    with conn.cursor() as cursor:\n",
    "        # Insert file metadata and content into the complete_files table\n",
    "        for chunk in doc_chunks:\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"INSERT INTO text_chunks (text) VALUES (%s) RETURNING id\"),\n",
    "                [chunk],\n",
    "            )\n",
    "            pk = cursor.fetchone()[0]  # Capture the returned primary key\n",
    "    \n",
    "            conn.commit()  # Commit the transaction after each insert\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting file with chunks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS re jupyter\n",
    "1. Exceptions?\n",
    "2. Connection?\n",
    "3. output = pprint?\n",
    "4. For codio - section limit?\n",
    "5. When we move it does Theme color matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Current Chunks from Database\n",
    "Create embedding of each chunk\n",
    "Store vector of embedding in pgvector (vector store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Retreive Currrent Chunks from Database\n",
    "current_chunks = []\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        sql.SQL(\"SELECT id, text FROM text_chunks WHERE is_vectorized = FALSE\"),\n",
    "    )\n",
    "\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        current_chunks.append(row) # append tuple of id and text \n",
    "        cursor.execute(f\"UPDATE text_chunks SET is_vectorized = TRUE WHERE id = %s\",\n",
    "                       (row[0],))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Get the embedding model\n",
    "openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Generate the embedding for each chunk\n",
    "vector_dict = {}\n",
    "for chunk in current_chunks:\n",
    "     content = openai_embedding.embed_query(chunk[1])\n",
    "     # Convert the embedding values to floats (ensures compatibility with storage formats)\n",
    "     float_content = [float(x) for x in content]\n",
    "     vector_dict[chunk[0]] = float_content\n",
    "\n",
    "\n",
    "\n",
    "# add the content to the vector store\n",
    "with conn.cursor() as cursor:\n",
    "    for cid, vec in vector_dict.items():\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"INSERT INTO mmr_vector (vector, text_chunk_id) VALUES (%s, %s)\"),\n",
    "            [vec,cid]\n",
    "        )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Incoming Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What was Net Interest Margin (NIM)?\"\n",
    "#query = \"How was fee income?\"\n",
    "vectorized_query = openai_embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Similar Vectors\n",
    "pgvector similarity search operators: \n",
    "<->:\n",
    "Represents the Euclidean distance between two vectors, which is the \"straight-line\" distance between them in multi-dimensional space. \n",
    "<=>:\n",
    "Calculates the cosine similarity between vectors, which is often preferred for high-dimensional data as it focuses on the angle between vectors rather than their magnitude. \n",
    "<#>\n",
    ": Computes the inner product of two vectors, where each corresponding element is multiplied and summed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"vectorized query: {vectorized_query[:5]}\")\n",
    "top_k = 3\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\n",
    "        sql.SQL(\n",
    "            \"\"\"SELECT id, text_chunk_id, 1 - (vector <#> %s::VECTOR) AS similarity\n",
    "               FROM mmr_vector\n",
    "               ORDER BY similarity DESC\n",
    "               LIMIT %s\"\"\"\n",
    "        ),\n",
    "        [vectorized_query, top_k],\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    similar_chunk_ids = []\n",
    "    if rows:\n",
    "        for row in rows:\n",
    "            similar_chunk_ids.append(row[1])\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the text chunks of the closest matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    similar_context = []\n",
    "    for chunk_id in similar_chunk_ids:\n",
    "        cur.execute(\n",
    "            sql.SQL(\"\"\"SELECT text FROM text_chunks where id = %s\"\"\"),\n",
    "            [chunk_id],\n",
    "        )\n",
    "        row = cur.fetchone()  # Fetch only one row for the current chunk_id\n",
    "        similar_context.append(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit Similar Vectors to LLM with query to retrieve result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Net Interest Margin (NIM) is a measure of the difference between the '\n",
      " 'interest income generated by banks or other financial institutions and the '\n",
      " 'amount of interest paid out to their lenders, relative to the amount of '\n",
      " 'their interest-earning assets. It is usually expressed as a percentage of '\n",
      " 'what the financial institution earns on loans in relation to the total '\n",
      " 'amount of these loans. In the provided context, the NIM was slightly above '\n",
      " 'target at 3.96%, which suggests it was driven by favorable economic '\n",
      " 'conditions and prudent financial management.')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "# Show the similar content retrieved\n",
    "# for sc in similar_context:\n",
    "#     pprint(f\"CONTEXT ITEM:{sc}\")\n",
    "# Format the prompt\n",
    "prompt = f\"\"\"You are an assistant for question-answering tasks. Use only \n",
    "the following pieces of retrieved context to answer the \n",
    "question. Use 3 sentences maximum to keep your answer concise. Here's a query: \n",
    "{query} and here are similar queries of retrieved context: {similar_context}. Again,\n",
    "only base your answer on the similar queries data within the similar context.\"\"\"\n",
    "\n",
    "# Call the OpenAI ChatCompletion API using the updated method\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "pprint(response.choices[0].message.content.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
