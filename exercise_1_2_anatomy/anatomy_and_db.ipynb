{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  Do we want to add text splitting? e.g. \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500, chunk_overlap=50\n",
    "        )\n",
    "        text_chunks_docx = text_splitter.split_text(\"\\n\".join(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE OVERVIEW\n",
    "\n",
    "In this exercise, we'll step through:\n",
    "A. Chunking a document into paragraphs\n",
    "B. Storing all of the raw paragraph text in our database\n",
    "C. After everything is stored as text in the database, we'll retrieve those paragraphs and embed/vectorize each one, storing it in the vector store (pgvector from Postgres in our case).\n",
    "D. Then, we'll act as a user and send in a query, that will also be vectorized by the same tool we used to vectorize our paragraphs.\n",
    "E. Then we'll find and retrieve the vectors that are most similar to our query.\n",
    "F. Once we have the similar vectors, we'll submit them, along with the user query to our LLM and print our response.\n",
    "\n",
    "\n",
    "NOTE: When you setup directories for labs, you may have to run this code to add the virtual environment as a kernel (otherwise, things worked but you will get some warnings).\n",
    "python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Import docx library for Python and instantiate a document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document as DocxDocument\n",
    "doc = DocxDocument(\"Jupyter_Notebook_Info.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Extract and chunk into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "for para in doc.paragraphs:\n",
    "            if para.text.strip():\n",
    "                doc_chunks.append(para.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 (First time only)\n",
    "\n",
    "You will want to make sure your env file has the proper database configuration info.\n",
    "\n",
    "If you have not yet ran create_db_python.py.  Take a minute to browse it and then run it please.\n",
    "This creates your relational database tables.\n",
    "\n",
    "The run create_pgvector_table.py.  This, creates the vector store table, mmr_vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Connect to the database \n",
    "(Database should now be created.  Se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "def get_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "                    dbname=os.getenv(\"dbname\"),\n",
    "                    user=os.getenv(\"dbuser\"),\n",
    "                    password=os.getenv(\"dbpassword\"),\n",
    "                    host=os.getenv(\"dbhost\"),\n",
    "                    port=os.getenv(\"dbport\"),\n",
    "                )\n",
    "    except (psycopg2.DatabaseError, Exception) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "    \n",
    "    return conn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Store the chunks in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import sql\n",
    "\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    with conn.cursor() as cursor:\n",
    "        # Insert file metadata and content into the complete_files table\n",
    "        for chunk in doc_chunks:\n",
    "            cursor.execute(\n",
    "                sql.SQL(\"INSERT INTO text_chunks (text) VALUES (%s) RETURNING pk\"),\n",
    "                [chunk],\n",
    "            )\n",
    "            pk = cursor.fetchone()[0]  # (Optional) capture the returned primary key\n",
    "    \n",
    "            conn.commit()  # Commit the transaction after each insert\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting file with chunks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 - Vectorization\n",
    "A. Retrieve Current Chunks from Database\n",
    "B. Create embedding of each chunk\n",
    "C. Store vector of embedding in pgvector (vector store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Retreive Currrent Chunks from Database\n",
    "current_chunks = []\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        sql.SQL(\"SELECT pk, text FROM text_chunks WHERE is_vectorized = FALSE\"),\n",
    "    )\n",
    "\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        current_chunks.append(row) # append tuple of id and text \n",
    "        cursor.execute(f\"UPDATE text_chunks SET is_vectorized = TRUE WHERE pk = %s\",\n",
    "                       (row[0],))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Get the embedding model\n",
    "openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=openai_key)\n",
    "\n",
    "# Generate the embedding for each chunk\n",
    "vector_dict = {}\n",
    "for chunk in current_chunks:\n",
    "     content = openai_embedding.embed_query(chunk[1])\n",
    "     # Convert the embedding values to floats (ensures compatibility with storage formats)\n",
    "     float_content = [float(x) for x in content]\n",
    "     vector_dict[chunk[0]] = float_content\n",
    "\n",
    "\n",
    "# add the vectorized content to the vector store\n",
    "with conn.cursor() as cursor:\n",
    "    chunk_type = \"text\"  # Only working with text right now, so hard-coding chunk_type = \"text\"\n",
    "    \n",
    "    for cid, vec in vector_dict.items():\n",
    "        cursor.execute(\n",
    "            sql.SQL(\"INSERT INTO mmr_vector (vector, chunk_type, chunk_id) VALUES (%s, %s, %s)\"),\n",
    "            [vec, chunk_type, cid]\n",
    "        )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 - Vectorize Incoming Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"Where should I point my web browser after Jupyter is running?\"\n",
    "# query = \"What is the command to install jupyter?\"\n",
    "# query = \"You can execute a cell by clicking on it and pressing what?\"\n",
    "# query = \"What City and State had the highest temperature?\"\n",
    "\n",
    "vectorized_query = openai_embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8 - Find Similar Vectors\n",
    "\n",
    "pgvector similarity search operators: \n",
    "<->:\n",
    "Represents the Euclidean distance between two vectors, which is the \"straight-line\" distance between them in multi-dimensional space. \n",
    "<=>:\n",
    "Calculates the cosine similarity between vectors, which is often preferred for high-dimensional data as it focuses on the angle between vectors rather than their magnitude. \n",
    "<#>\n",
    ": Computes the inner product of two vectors, where each corresponding element is multiplied and summed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"vectorized query: {vectorized_query[:5]}\")\n",
    "top_k = 3\n",
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\n",
    "        sql.SQL(\n",
    "            \"\"\"SELECT pk, chunk_type, chunk_id, 1 - (vector <=> %s::VECTOR) AS similarity\n",
    "               FROM mmr_vector\n",
    "               ORDER BY similarity DESC\n",
    "               LIMIT %s\"\"\"\n",
    "        ),\n",
    "        [vectorized_query, top_k],\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    similar_chunk_ids = []\n",
    "    if rows:\n",
    "        for row in rows:\n",
    "            similar_chunk_ids.append(row[2])\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9 - Get the text chunks of the closest matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar_chunk_ids:[2, 47, 92]\n"
     ]
    }
   ],
   "source": [
    "conn = get_connection()\n",
    "with conn.cursor() as cur:\n",
    "    print(f\"similar_chunk_ids:{similar_chunk_ids}\")\n",
    "    similar_context = []\n",
    "    for chunk_id in similar_chunk_ids:\n",
    "        cur.execute(\n",
    "            sql.SQL(\"\"\"SELECT text FROM text_chunks where pk = %s\"\"\"),\n",
    "            [chunk_id],\n",
    "        )\n",
    "        row = cur.fetchone()  # Fetch only one row for the current chunk_id\n",
    "        similar_context.append(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Retrieval\n",
    "Submit Similar Vectors to LLM with query to retrieve a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CONTEXT ITEM:Once Jupyter is running, point your web browser at '\n",
      " 'http://localhost:8888 to start using Jupyter notebooks. If everything worked '\n",
      " 'correctly, you should see a screen like this, showing all available Jupyter '\n",
      " 'notebooks in the current directory:')\n",
      "('CONTEXT ITEM:Once Jupyter is running, point your web browser at '\n",
      " 'http://localhost:8888 to start using Jupyter notebooks. If everything worked '\n",
      " 'correctly, you should see a screen like this, showing all available Jupyter '\n",
      " 'notebooks in the current directory:')\n",
      "('CONTEXT ITEM:Once Jupyter is running, point your web browser at '\n",
      " 'http://localhost:8888 to start using Jupyter notebooks. If everything worked '\n",
      " 'correctly, you should see a screen like this, showing all available Jupyter '\n",
      " 'notebooks in the current directory:')\n",
      "\n",
      "\n",
      "\n",
      "('RESPONSE:After Jupyter is running, you should point your web browser at '\n",
      " 'http://localhost:8888 to start using Jupyter notebooks. If everything worked '\n",
      " 'correctly, you will see a screen showing all available Jupyter notebooks in '\n",
      " 'the current directory.')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "# Show the similar content retrieved\n",
    "for sc in similar_context:\n",
    "    pprint(f\"CONTEXT ITEM:{sc}\")\n",
    "\n",
    "# Format the prompt\n",
    "prompt = f\"\"\"You are an assistant for question-answering tasks. Use only \n",
    "the following pieces of retrieved context to answer the \n",
    "question. Use 3 sentences maximum to keep your answer concise. Here's a query: \n",
    "{query} and here are similar queries of retrieved context: {similar_context}. Again,\n",
    "only base your answer on the similar queries data within the similar context.\"\"\"\n",
    "\n",
    "# Call the OpenAI ChatCompletion API using the updated method\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "print(\"\\n\\n\")\n",
    "pprint(f\"RESPONSE:{response.choices[0].message.content.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
