{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying and Ingesting Your Data**\n",
    "\n",
    "In this exercise:  \n",
    "We'll continue to work with our document.  \n",
    "Step 1: Extract our data into paragraphs.  \n",
    "Step 2: Split the paragraphs into chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extract our data into paragraphs.  \n",
    "\n",
    "Remember that the LLM will tokenize our content.  Should we break it into pages, paragraph, sentences, or something else?   (Could even convert to HTML and then parse.)  \n",
    "[Tokenizer Example](../static/images/tokenizer.png)  \n",
    "[Python Docx API](https://python-docx.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document as DocxDocument\n",
    "\n",
    "file_path = \"../static/input_files/Jupyter_Notebook_Info.docx\"\n",
    "doc = DocxDocument(file_path)\n",
    "paragraph_chunks = []\n",
    "\n",
    "# Extract text content from paraAgraphs\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.strip():\n",
    "        paragraph_chunks.append(para.text)\n",
    "        # print(f\"PARAGRAPH:{para.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Split the paragraphs into chunks.  \n",
    "Discuss: What's the best way?  \n",
    "\n",
    "[Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# We join all the text into a single string before splitting\n",
    "text_chunks = text_splitter.split_text(\"\\n\".join(paragraph_chunks))\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    print(f\"CHUNK:{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<u style=\"color: yellow;\">_________________________________________________________________________________________</u>  \n",
    "  \n",
    "**Process and Transform Data**  \n",
    "\n",
    "In this exercise:  \n",
    "Step 1: Discuss Text Cleaning and Preprocessing Considerations.  \n",
    "Step 2: Summarize our text chunks.  \n",
    "Step 3: Vectorize each summarized text chunk.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Text Cleaning and Preprocessing Considerations:  \n",
    "* Handle Whitespace  \n",
    "* Lowercase  \n",
    "* Remove Punctuation (sometimes, but be careful)  \n",
    "* Remove Special Characters  \n",
    "* Handle HTML/XML Tags if needed  \n",
    "* Handle Accents and Diacritics  \n",
    "* Expand Contractions  \n",
    "* Stop Word Removal (e.g., \"the,\" \"a,\" \"is\")  \n",
    "* Stemming / Lemmatization (Usually Lemmatization is preferred)  \n",
    "* Consider Metadata  \n",
    "* Consider Summarization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Summarization  \n",
    "[Langchain ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai/)  \n",
    "[Langchain ChatPromptTemplate](https://sj-langchain.readthedocs.io/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html)  \n",
    "[Langchain HumanMessage](https://sj-langchain.readthedocs.io/en/latest/schema/langchain.schema.messages.HumanMessage.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the ChatGPT model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Create the text prompt (note the {element} variable)\n",
    "text_prompt_text = \"\"\"\n",
    "        You are an assistant tasked with summarizing text for semantic retrieval.\n",
    "        These summaries will be embedded and used to retrieve the raw text elements.\n",
    "        Give a detailed summary of the text below that is well optimized for retrieval.\n",
    "        Also, provide a one-line description of what the text is about.\n",
    "        Do not add additional words like Summary: etc.\n",
    "        Text chunk:\n",
    "        {element}\n",
    "\"\"\"\n",
    "\n",
    "text_prompt = ChatPromptTemplate.from_template(text_prompt_text)\n",
    "\n",
    "# Create the Summary\n",
    "summaries = []\n",
    "\n",
    "for text in text_chunks:\n",
    "    prompt_text = text_prompt.format(element=text) # pass text chunk into prompt\n",
    "    response = llm.invoke([HumanMessage(content=prompt_text)])\n",
    "    summaries.append(response.content)\n",
    "\n",
    "for summary in summaries:\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Vectorize each summarized chunk.  \n",
    "\n",
    "[Langchain OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0757230594754219, -0.01474311575293541, 0.03324216231703758, -0.01488342322409153, -0.00787342805415392]\n",
      "[-0.04038451611995697, 0.03060675784945488, 0.05201853811740875, -0.02758493460714817, -0.0067289541475474834]\n",
      "[-0.01937182806432247, 0.047913625836372375, 0.05982820689678192, -0.022956840693950653, -0.0385521724820137]\n",
      "[-0.011679466813802719, 0.040910590440034866, 0.06260524690151215, 0.008179757744073868, -0.020018570125102997]\n",
      "[-0.006696468219161034, 0.045008622109889984, 0.056652382016181946, 0.013334195129573345, 0.01661716215312481]\n",
      "[-0.005106356460601091, 0.029977384954690933, 0.021410761401057243, -0.026615651324391365, 0.005822173785418272]\n",
      "[0.005611640866845846, -0.0045518409460783005, 0.0349310077726841, -0.027173271402716637, 0.0020202435553073883]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def get_embedding(text):\n",
    "    openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    content = openai_embedding.embed_query(text)\n",
    "    float_content = [float(x) for x in content] # needed for pgvector\n",
    "    return float_content\n",
    "\n",
    "for summary in summaries:\n",
    "    embedded_summary = get_embedding(summary)\n",
    "    print(embedded_summary[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
